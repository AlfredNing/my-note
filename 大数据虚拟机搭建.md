### 大数据环境搭建

#### 虚拟机准备

1. 安装vmware15.6一路next

2. 配置网络编辑器
   ![image-20211110083124839](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083124839.png)

3. Vmnet8, 设置IP
   ![image-20211110083234528](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083234528.png)

   ![image-20211110083327778](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083327778.png)

4. 修改网络适配器

   ![image-20211110083537281](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083537281.png)

   ![image-20211110083611960](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083611960.png)



​				![	](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110083808977.png)

5. **网络编辑器配置成功**

#### centos7安装

1. 使用版本：CentOS-7-x86_64-DVD-2009

2. 初始化虚拟机，推荐+next

3. 安装操作系统

   1. 开机别测试了，直接installing

   2. 选择语言，看你爱好

   3. 配置

      1. ![image-20211110232044105](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232044105.png)
      2. ![image-20211110232110662](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232110662.png)
      3. ![image-20211110232120324](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232120324.png)
      4. ![image-20211110232131456](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232131456.png)
      5. ![image-20211110232148845](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232148845.png)
      6. ![image-20211110232154668](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232154668.png)
      7. ![image-20211110232201558](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232201558.png)
      8. ![image-20211110232207710](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232207710.png)
      9. ![image-20211110232213642](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232213642.png)
      10. ![image-20211110232224628](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232224628.png)
      11. ![image-20211110232230354](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232230354.png)
      12. ![image-20211110232238508](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232238508.png)
      13. ![image-20211110232242722](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232242722.png)
      14. ![image-20211110232248147](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232248147.png)

   4. ![image-20211110232254730](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211110232254730.png)

   5. 设置用户密码
      **root ======> ningqiang**

   6. 等待，重启开机

   7. ![image-20211111001319434](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211111001319434.png)

   8. ![image-20211111001331804](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211111001331804.png)

   9. 一路设置简单

   10. 用户：**nq=====>ningqiang**

   11. 虚拟机网络ip配置（**root用户**）

       1.  vi /etc/sysconfig/network-scripts/ifcfg-ens33

          ```bash
          E="Ethernet"
          PROXY_METHOD="none"
          BROWSER_ONLY="no"
          BOOTPROTO="static"
          DEFROUTE="yes"
          IPV4_FAILURE_FATAL="no"
          IPV6INIT="yes"
          IPV6_AUTOCONF="yes"
          IPV6_DEFROUTE="yes"
          IPV6_FAILURE_FATAL="no"
          IPV6_ADDR_GEN_MODE="stable-privacy"
          NAME="ens33"
          UUID="fd649c79-80e4-42ce-a9c3-8e843bb9e964"
          DEVICE="ens33"
          ONBOOT="yes"
          IPADDR=192.168.10.100
          GATEWAY=192.168.10.2
          DNS1=192.168.10.2
          DNS2=8.8.8.8
          ```

       2. 重启网卡
          `systemctl restart network`

       3. 检查ip

          `ip addr` 或者`ifconfig`

   12. 修改主机名称
       `vi /etc/hostname`

   13. 如果要配置克隆虚拟机，本机是一台纯正的虚拟机不配置
       `vi /etc/hosts`

   14. 配置更好yum源

       1. 确保 wget安装： `yum install -y  wget`

       2. 保留本地yum 

          ` mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak`

       3. 配置阿里源
          `cd /etc/yum.repos.d`
          ` wget -nc http://mirrors.aliyun.com/repo/Centos-7.repo`

       4. 更新本地yum缓存

          ```bash
          # 全部清除
          sudo yum clean all
          # 更新列表
          sudo yum list
          # 缓存yum包信息到本机，提高搜索速度
          sudo yum makecache
          ```

   15. 关闭网络防火墙
       `systemctl stop firewalld`

       `systemctl disable firewalld.service //禁止开启自动启动`

   16. 配置nq用户具有root权限

       ```bash
       vim /etc/sudoers
       ```

       ![image-20211113090438724](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20211113090438724.png)

   17. 安装工具 

       ```bash
       yum install -y epel-release
       yum install -y net-tools
       yum install -y vim
       ```

   18. 创建需要的文件夹，并修改用户和用户组

       ```bash
       mkdir /opt/module
       mkdir /opt/software
       chown nq:nq /opt/module
       chown nq:nq /opt/software
       ```

   19. 卸载虚拟机自带jdk

       ```bash
       rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps
       ```

   20. 准备完毕

#### 克隆虚拟机

1. 原虚拟机需要在关闭状态，右击克隆

2. 修改网络防火墙
   `vim /etc/sysconfig/network-scripts/ifcfg-ens33`

   ```bash
   TYPE="Ethernet"
   PROXY_METHOD="none"
   BROWSER_ONLY="no"
   BOOTPROTO="static"
   DEFROUTE="yes"
   IPV4_FAILURE_FATAL="no"
   IPV6INIT="yes"
   IPV6_AUTOCONF="yes"
   IPV6_DEFROUTE="yes"
   IPV6_FAILURE_FATAL="no"
   IPV6_ADDR_GEN_MODE="stable-privacy"
   NAME="ens33"
   DEVICE="ens33"
   ONBOOT="yes"
   IPADDR=192.168.10.101
   GATEWAY=192.168.10.2
   DNS1=192.168.10.2
   DNS2=8.8.8.8  
   ```

3. 修改主机名

   ```bash
   vim /etc/hostname
   
   // hadoop01
   ```
   
4. 配合本地客户端链接，选择工具有，Xsheell/SecureCRT/MobaXterm,自行选择

#### 安装jdk

1. 上传文件
2. 解压

`tar -zxvf jdk-8u231-linux-x64.tar.gz -C  ../module/`

3. 配置环境变量
   `vim /etc/profile.d/my_env.sh`

   ```bash
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_231
   export PATH=$PATH:$JAVA_HOME/bin
   ```

4. `source /etc/profile`

#### hadoop搭建

1. 上传hadoop2.7.2 至服务器

2. 修改配置文件
   **core.site.xml**

   ```xml
   <!-- 指定HDFS中NameNode的地址 -->
   <property>
   		<name>fs.defaultFS</name>
         <value>hdfs://hadoop111:9000</value>
   </property>
   
   <!-- 指定Hadoop运行时产生文件的存储目录 -->
   <property>
   		<name>hadoop.tmp.dir</name>
   		<value>/opt/module/hadoop-2.7.2/data/tmp</value>
   </property>
   
   hadoop-env.sh
   export JAVA_HOME=/opt/module/jdk1.8.0_231
   
   hdfs-site.xml
   <property>
   		<name>dfs.replication</name>
   		<value>3</value>
   </property>
   
   <!-- 指定Hadoop辅助名称节点主机配置 -->
   <property>
         <name>dfs.namenode.secondary.http-address</name>
         <value>hadoop113:50090</value>
   </property>
   
   yarn-env.sh
   export JAVA_HOME=/opt/module/jdk1.8.0_231
   
   
   yarn-site.xm
   <!-- Reducer获取数据的方式 -->
   <property>
   		<name>yarn.nodemanager.aux-services</name>
   		<value>mapreduce_shuffle</value>
   </property>
   
   <!-- 指定YARN的ResourceManager的地址 -->
   <property>
   		<name>yarn.resourcemanager.hostname</name>
   		<value>hadoop112</value>
   </property>
   
   mapred-env.sh
   export JAVA_HOME=/opt/module/jdk1.8.0_231
   
   mapred-site.xml
   <property>
   		<name>mapreduce.framework.name</name>
   		<value>yarn</value>
   </property>
   
   
   slaves
   hadoop111
   hadoop112
   hadoop113
   ```

   3. 分发文件

   4. 第一次启动
      `bin/hdfs namenode -format`

   5. 注意：NameNode和ResourceManger如果不是同一台机器，不能在NameNode上启动 YARN，应该在ResouceManager所在的机器上启动YARN

   6. hdfs
      `sbin/start-dfs.sh`
      [Namenode information](http://192.168.10.111:50070/dfshealth.html#tab-overview)
      [SecondaryNamenode information](http://192.168.10.113:50090/status.html)

   7. yarn
      `sbin/start-yarn.sh`

      [All Applications](http://192.168.10.112:8088/cluster)


#### hive搭建

1. 3台虚拟机，安装了Hadoop

2. ```shell
   # 查询是否安装了mariadb
   rpm -aq | grep mariadb
   # 删除mariadb。-e 删除指定的套件；--nodeps 不验证套件的相互关联性
   rpm -e --nodeps mariadb-libs
   ```

3. 安装依赖

   ```shell
   yum install perl -y
   yum install net-tools -y
   ```

4. 安装mysql

   ```shell
   # 接压缩
   tar xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar
   # 依次运行以下命令
   rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm
   rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm
   rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm
   rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm
   
   # 启动mysql
   systemctl start mysqld
   ```

5. 创建hive用户

   ```shell
   -- 创建用户设置口令、授权、刷新
   CREATE USER 'hive'@'%' IDENTIFIED BY '12345678';
   GRANT ALL ON *.* TO 'hive'@'%';
   FLUSH PRIVILEGES;
   ```

6. 安装hive

   - 解压缩

   - 修改环境变量

      ```shell
      tar -zxf apache-hive-2.3.7-bin.tar.gz  -C ../module/
      mv  apache-hive-2.3.7-bin/ apache-hive-2.3.7
      vim /etc/profile.d/my_env.sh
      
      
      export HIVE_HOME=/opt/ module/apache-hive-2.3.7
      export PATH=$PATH:$HIVE_HOME/bin
      source /etc/profile
      
      ```

7. 修改 Hive 配置
   **cd $HIVE_HOME/conf vi hive-site.xml** 
   cp hive-default.xml.template hive-site.xml

   ```xml
   <configuration>
   <!-- hive元数据的存储位置 -->
   <property>
   <name>javax.jdo.option.ConnectionURL</name>
   <value>jdbc:mysql://linux123:3306/hivemetadata?
   createDatabaseIfNotExist=true&amp;useSSL=false</value>
   <description>JDBC connect string for a JDBC
   metastore</description>
   </property>
   <!-- 指定驱动程序 -->
   <property>
   <name>javax.jdo.option.ConnectionDriverName</name>
   <value>com.mysql.jdbc.Driver</value>
   <description>Driver class name for a JDBC
   metastore</description>
   </property>
   <!-- 连接数据库的用户名 -->
   <property>
   <name>javax.jdo.option.ConnectionUserName</name>
   <value>hive</value>
   <description>username to use against metastore
   database</description>
   </property>
   <!-- 连接数据库的口令 -->
   <property>
   <name>javax.jdo.option.ConnectionPassword</name>
   <value>12345678</value>
   <description>password to use against metastore
   database</description>
   </property>
   </configuration>
   
   注意jdbc的连接串，如果没有 useSSL=false 会有大量警告
   在xml文件中 &amp; 表示 &
   ```

8. 拷贝 MySQL JDBC 驱动程序

9. 初始化元数据库
   **schematool -dbType mysql -initSchema**

10. 启动Hive，执行命令

    ```shell
    # 启动hive服务之前，请先启动hdfs、yarn的服务
    hive
    
    ```


#### zookeeper集群搭建

~~前提准备三台虚拟机，ssh，hosts~~

1. 目录准备

   - 创建数据存储目录：mkdir -p /opt/module/zookeeper-3.4.14/data
   - 创建日志配置文件：mkdir -p /opt/module/zookeeper-3.4.14/data/logs

2. conf/ 下 `mv zoo_sample.cfg zoo.cfg`

3. vim zoo.cfg

   ```shell
   vim zoo.cfg
   #datadir
   dataDir=/opt/module/zookeeper-3.4.14/data
   #logdir
   dataLogDir=/opt/module/zookeeper-3.4.14/data/logs
   #增加集群配置
   ##server
   server.1=kafka-node01:2888:3888
   server.2=kafka-node02:2888:3888
   server.3=kafka-node03:2888:3888
   
   #ZK自动清理事务日志和快照文件功能，清理频率，默认1小时
   autopurge.purgeInterval=1
   ```

4. data目录下创建名为myid文件，内容为1 集群id不能相同

#### Kafka集群搭建

1. 前提三台虚拟机配置java环境，ssh, zookeeper集群已经搭建

2. 修改conf/server.properties

   ```shell
   broker.id=0 # 每台机器唯一
   listeners=PLAINTEXT://:9092
   advertised.listeners=PLAINTEXT://node2:9092 # 主机端口号写当前服务器
   log.dirs=/var/lagou/kafka/kafka-logs
   zookeeper.connect=node2:2181,node3:2181,node4:2181/myKafka # zookeeper连接信息
   ```

#### linux mysql 搭建

```shell
# 前提清理安装的mysql文件 
# 使用版本 mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar
rpm -e --nodeps mariadb-libs

# 解压缩
tar xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar

# 依次运行一下命令
rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm

# 启动数据库
systemctl start mysqld
# 查找临时密码
grep password /var/log/mysqld.log
# 修改root口令
mysql -uroot -p 临时

set global validate_password_policy=0;
set password for 'root'@'localhost' =password('12345678');
flush privileges;

# 远程连接
update user set host='%' where user = 'root'; # 或者上面改成%
flush privileges;
```

```shell
# 卸载mysql
rpm -qa |grep mysql
systemctl stop mysqld
rpm -e --nodeps 安装包XXX
# 删除文件
rm -rf /var/lib/mysql
rm -rf /etc/my.cnf

userdel mysql
groupdel mysql

## 确认卸载完毕
rpm -qa|grep -i mysql

```



#### canal搭建

<u>前提下载好canal 配置mysql</u> 

1. mysql 开启binglog

   ```shell
   vim /etc/my.cnf
   server-id=1
   log-bin=mysql-bin
   binlog-format=ROW
   binlog-do-db=dwshow
   
   # restart mysql
   systemctl restart mysqld
   
   # 是否启用binlog日志
   show variables like 'log_bin';
   # 查看binlog类型
   show global variables like 'binlog_format';
   # 查看详细的日志配置信息
   show global variables like '%log%';
   # mysql数据存储目录
   variables like '%dir%';
   # 查看binlog的目录
   show global variables like "%log_bin%";
   # 查看当前服务器使用的biglog文件及大小
   show binary logs;
   # 查看最新一个binlog日志文件名称和Position
   show master status;
   # 查询binlog 变动信息
   show binlog events;
   ```

2. 创建canal用户

   ```mysql
   GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%'
   IDENTIFIED BY 'canal' ;
   
   ## waring: 注意可能会遇到密码设置问题
   set global validate_password_length = 4;
   SET GLOBAL validate_password_number_count = 0;
   ```

3. canal 配置文件 conf/canal.properties

   ```properties
   # 配置zookeeper地址
   canal.zkServers =linux121:2181,linux123:2181
   # tcp, kafka, RocketMQ
   canal.serverMode = kafka
   # 配置kafka地址
   canal.mq.servers =linux121:9092,linux123:9092
   ```

4. canal配置文件conf/example/instance.properties

   ```properties
   # 配置MySQL数据库所在的主机
   canal.instance.master.address = linux123:3306
   # username/password，配置数据库用户和密码
   canal.instance.dbUsername =canal
   canal.instance.dbPassword =canal
   # mq config,对应Kafka主题：
   canal.mq.topic=yzx
   ```

5. 启动测试

   **这里如果有意外，重启虚拟机，重新安装canal**

#### 普罗米修斯Prometheus安装

>与exporter连用，集成其他监控系统

##### 安装go环境

`tar -C /usr/local -xzf go1.8.3.linux-amd64.tar.gz`

**https://storage.googleapis.com/golang/go1.8.3.linux-amd64.tar.gz**

配置环境变量`export PATH=$PATH:/usr/local/go/bin（选择自己的路径，和自己的路径对应）`

```shell
#go_home
export GO_HOME=/opt/module/apps/go
export GOROOT=/opt/module/apps/go
export PATH=$PATH:$GO_HOME/bin:$GOROOT/bin
```



验证：**go version**

##### 在监控服务器上安装prometheus

https://github.com/prometheus/prometheus/releases/tag/v2.22.1

1. `tar -zvxf prometheus-2.22.1.linux-amd64.tar.gz -C /root/apps`

2. 配置监控文件：prometheus.yml

   ```shell
   # my global config
   global:
     scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
     evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
     # scrape_timeout is set to the global default (10s).
   
   # Alertmanager configuration
   alerting:
     alertmanagers:
     - static_configs:
       - targets:
         # - alertmanager:9093
   
   # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
   rule_files:
     # - "first_rules.yml"
     # - "second_rules.yml"
   
   # A scrape configuration containing exactly one endpoint to scrape:
   # Here it's Prometheus itself.
   scrape_configs:
     # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
     - job_name: 'prometheus'
   
       # metrics_path defaults to '/metrics'
       # scheme defaults to 'http'.
   
       static_configs:
       - targets: ['localhost:9090']
     - job_name: 'bigdata-129'
       static_configs:
       - targets: ['192.168.10.114:9100']
                                                 
   ```

   3. ./prometheus
   4. 验证：**主机：9090**地址

##### node_exporter + pushgateway安装

**node_exporter 安装**

1. tar xvf node_exporter-1.0.1.linux-amd64.tar.gz -C /root/apps
2. nohup /usr/local/node_exporter-1.0.1.linux-amd64/node_exporter &

3. 验证：[192.168.10.114:9100/metrics](http://192.168.10.114:9100/metrics)

**pushgateway 安装**

1. 修改flink配置，开放flink被监控端口

   1. 把prometheus的jar包复制到flink的lib目录下 （在flink/puugin目录下）

   2. 在flink-conf.yaml增加如下内容，修改：
      metrics.reporter.promgateway.host: hdp-1

      ```shell
      metrics.reporter.promgateway.class:
      org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter
      metrics.reporter.promgateway.host: hdp-1
      metrics.reporter.promgateway.port: 9091
      metrics.reporter.promgateway.jobName: myJob
      metrics.reporter.promgateway.randomJobNameSuffix: true
      metrics.reporter.promgateway.deleteOnShutdown: false
      ```

2. 增加pushgateway

   1. 解压安装：chmod +x pushgateway
      

      ```shell
      vi prometheus.yml
      - job_name: 'bigdata-grafana'
      static_configs:
      - targets: ['192.168.81.129:9091']
      ```

##### grafana可视化结果

#### ClickHouse安装

1. 上传四个文件到虚拟机

   ![image-20220309142650574](C:\Users\nq\AppData\Roaming\Typora\typora-user-images\image-20220309142650574.png)

**单机模式 ==》 分别安装4个rpm,启动ClickServer**

**集群安装**

1. 上传四个文件到三台虚拟机

   执行安装 `rpm -ivh ./*.rpm`

2. 修改配置文件

   `vim /etc/clickhouse-server/config.xml`

   注意点：

   ```shell
   <!-- Path to data directory, with trailing slash. -->
   <path>/var/lib/clickhouse/</path>
   
   zookeeper标签上面增加：
   <include_from>/etc/clickhouse-server/config.d/metrika.xml</include_from>
   ```

3. 在三台机器的 d目录下新建metrika.xml文件

   ```xml
   <yandex>
       <clickhouse_remote_servers>
           <perftest_3shards_1replicas>
               <shard>
                   <internal_replication>true</internal_replication>
                   <replica>
                       <host>hadoop111</host>
                       <port>9000</port>
                   </replica>
               </shard>
               <shard>
                   <replica>
                       <internal_replication>true</internal_replication>
                       <host>hadoop112</host>
                       <port>9000</port>
                   </replica>
               </shard>
               <shard>
                   <internal_replication>true</internal_replication>
                   <replica>
                       <host>hadoop113</host>
                       <port>9000</port>
                   </replica>
               </shard>
           </perftest_3shards_1replicas>
       </clickhouse_remote_servers>
       <zookeeper-servers>
           <node index="1">
               <host>hadoop111</host>
               <port>2181</port>
           </node>
           <node index="2">
               <host>hadoop112</host>
               <port>2181</port>
           </node>
           <node index="3">
               <host>hadoop113</host>
               <port>2181</port>
           </node>
       </zookeeper-servers>
       <macros>
           <shard>01</shard>
           <replica>hadoop111</replica>
       </macros>
       <networks>
           <ip>::/0</ip>
       </networks>
       <clickhouse_compression>
           <case>
               <min_part_size>10000000000</min_part_size>
               <min_part_size_ratio>0.01</min_part_size_ratio>
               <method>lz4</method>
           </case>
       </clickhouse_compression>
   </yandex>
   ```

4. 启动zookeeper

5. 启动clickhouse 

    `systemctl start clickhouse-server`

6. 验证启动成功
   `clickhouse-client -m`

#### Kudo安装

hadoop111 hadoop112 hadoop113 112作为master

>hadoop111 意外yum失效，剔除

##### 安装ntp服务

```shell
yum -y install ntp

vi /etc/ntp.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst

restrict 192.168.10.0 mask 255.255.255.0 notrap nomodify # 给192.168.81.0网段，子网掩码为255.255.255.0的局域网机的机器有同步时间的权限
server 192.168.10.112 prefer # prefer代表优先使用此ip做同步
server 127.127.1.0 # 当所有服务器都不能使用时，使用本机作为同步服务器
fudge 127.127.1.0 stratum 10

service ntpd start
service ntpd status
```



##### 配置kudo

```shell
wget http://archive.cloudera.com/kudu/redhat/7/x86_64/kudu/cloudera-kudu.repo

mv cloudera-kudu.repo mv /etc/yum.repos.d/
```

1. 安装

```shell
# 每个节点执行
yum install kudu kudu-master kudu-client0 kudu-client-devel -y
```

2. 配置master

```shell
vi /etc/default/kudu-master

export FLAGS_rpc_bind_addresses=192.168.10.112:7051
```

3. 修改每个节点的kudu-tserver启动配置

```shell
vi /etc/default/kudu-tserver
export FLAGS_rpc_bind_addresses=192.168.10.112:7050
```

4. master.gflagfile的配置修改

```shell
--fromenv=rpc_bind_addresses
--fromenv=log_dir
--fs_wal_dir=/var/lib/kudu/master
--fs_data_dirs=/var/lib/kudu/master
-unlock_unsafe_flags=true
-allow_unsafe_replication_factor=true
-default_num_replicas=1 # 此参数可以调整备份数量，默认为3
```

5. tserver.gflagfile 的配置修改

```shell
# Do not modify these two lines. If you wish to change these variables,
# modify them in /etc/default/kudu-tserver.
--fromenv=rpc_bind_addresses
--fromenv=log_dir
--fs_wal_dir=/var/lib/kudu/tserver
--fs_data_dirs=/var/lib/kudu/tserver
--tserver_master_addrs=hdp-2:7051
-unlock_unsafe_flags=true
-allow_unsafe_replication_factor=true
-default_num_replicas=1
--tserver_master_addrs=192.168.81.130:7051 # 此参数指定master
```

注意，这里的–tserver_master_addrs指明了集群中master的地址，指向同一个master的tserver形成了一
个kudu集群

6. 创建master.gflagfile和tserver.gflagfile文件中指定的目录，并将所有者更改为kudu，执行如下命令

```shell
mkdir -p /var/lib/kudu/master /var/lib/kudu/tserver
chown -R kudu:kudu /var/lib/kudu/
```

7. 修改 /etc/security/limits.d/20-nproc.conf 文件，解除kudu用户的线程限制,注意：20可能不同，根据自
   己的来修改

```she
vi /etc/security/limits.d/20-nproc.conf

kudu soft nproc unlimited
impala soft nproc unlimited
```

8. 启动kudu

```shell
service kudu-master start
service kudu-tserver start  # slaver节点执行

##### 问题
发现时间问题，解决方案，重启ntpd service ntpd restart
然后重启kudu-master service kudu-master restart
```





### 脚本

前提配置ssh

#### zookeeper

集群脚本

```shell
#!/bin/bash
echo "====集群启动zookeeper===="
if(($#==0));then
echo "no params";
exit;
fi

hosts="kafka-node01 kafka-node02 kafka-node03"

for host in $hosts
do
ssh $host "/opt/module/zookeeper-3.4.14/bin/zkServer.sh $1"
done
```

**chmod u + x scripts;**

#### kafka

启动脚本

```shell
echo "====集群启动kafka===="

sh /opt/module/kafka_2.12-3.0.0/bin/kafka-server-start.sh /opt/module/kafka_2.12-3.0.0/config/server.properties &
ssh kafka-node02 "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-start.sh /opt/module/kafka_2.12-3.0.0/config/server.properties &"
ssh kafka-node03 "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-start.sh /opt/module/kafka_2.12-3.0.0/config/server.properties &"

```

关闭脚本

```shell
echo "====集群关闭kafka===="

sh /opt/module/kafka_2.12-3.0.0/bin/kafka-server-stop.sh
ssh kafka-node02 "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-stop.sh"
ssh kafka-node03 "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-stop.sh"
```



#### 附录

|      | hadoop111 | hadoop112 | hadoop113 |
| ---- | --------- | --------- | --------- |
|      |           |           |           |
|      |           |           |           |
|      |           |           |           |

git config --global http.postBuffer 531044600

git config http.sslVerify "false"

git config --global sendpack.sideband false

git config --global http.lowSpeedLimit 0
git config --global http.lowSpeedTime 999999

git config --global --unset http.proxy 
git config --global --unset https.proxy
