## 大数据面试

### HDFS

#### hdfs写数据流程

1. 客户端通过Distributed FileSystem 模块向NameNode 请求上传文件，NameNode**检查目标文件**是否存在，**父目录**是否存在
2. NameNode返回是否可以上传
3. 客户端请求第一个Block上传到哪几个DataNode服务器上
4. NameNode返回DataNode节点，返回节点地址
5. 客户端通过FSDataOutputStream模块请求向dn1上传数据，dn1节点收到请求，持续调用dn2,dn3
6. dn1,dn2,dn3逐级应答客户端
7. 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到本地内存缓存)，以packet为单位，dn1收到第一个packet就会传给dn2,dn3,dn1每传一个packet会放入一个应答队列等待应答
8. 当第一个Block传输完成之后，客户端请求NameNode上传第二个Block服务器，重复执行3-7，

#### hdfs读数据流程

1. 客户端通过Distributed FileSystem模块请求向NameNode下载文件，NameNode通过查询元数据，找到文件所在的DataNode节点地址
2. 挑选一台DataNode节点（就近原则，随机）服务器,，请求读取数据
3. DataNode开始传输数据给客户端，（从磁盘里面开始读取输入流，以Packet为单位校验）
4. 客户端以Packet为单位接受，先在本地缓存，然后写入目标文件

### Hive

将 SQL 转换为 MapReduce 任务的工具,可以将结构化的数据文件映射为一张表

**Hive的log默认存放在 /tmp/root 目录下**

**Hadoop 2.x 中 NameNode RPC缺省的端口号：8020**

#### 内部表与外部表

- 默认情况下，创建内部表。如果要创建外部表，需要使用关键字 external
- 在删除内部表时，表的定义(元数据) 和 数据 同时被删除
- 在删除外部表时，仅删除表的定义，数据被保留
- 在生产环境中，多使用外部表

#### UDTF，UDF,UDAF函数

UDF（User Defined Function）。用户自定义函数，一进一出

UDTF : User Defined Table-Generating Functions。用户定义表生成函数，一行输
入，多行输出。类似于：explode

UDAF（User Defined Aggregation Function）。用户自定义聚集函数，多进一
出；类似于：count/max/min

1.UDF

- 继承org.apache.hadoop.hive.ql.exec.UDF
- 需要实现evaluate函数；evaluate函数支持重载
- UDF必须要有返回类型，可以返回null，但是返回类型不能为void

### Hue

类似的产品还有 Apache Zeppelin

Hue是一个友好的界面集成框架，可以集成我们各种学习过的以及将要
学习的框架，一个界面就可以做到查看以及执行所有的框架

### Flume

是一个分布式、高可靠、高可用的海量日志采集、聚合、传输的系统

支持在日志系统中定制各类数据发送方，用于采集数据

Flume有3个重要组件：**Source、Channel、Sink**

其他数据采集工具还有：dataX、kettle、Logstash、Scribe、sqoop

### Sqoop

> CDC change data capture

Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库（mysql、postgresql等）间进行数据的传递。可以将关系型数据库（MySQL ,Oracle,Postgres等）中的数据导入到HDFS中，也可以将HDFS的数据导进到关系型数据库中。	

### 手撕算法

#### 冒泡排序 

> 冒泡特点：每次找到最大值/最小值 升序/降序排序

```java
public class BubbleSort {
    public static void bubbleSort(int[] data) {
        int length = data.length;
        for (int i = 0; i < length - 1; i++) {  // 循环次数
            for (int j = 0; j < length - 1 - i; j++) {
                if (data[j] > data[j + 1]) {
                    swap(data, j);
                }
            }
        }
    }

    private static void swap(int[] data, int j) {
        int tmp = data[j + 1];
        data[j+1] = data[j];
        data[j] = tmp;
    }

    public static void main(String[] args) {
        int[] data = { 9, -16, 21, 23, -30, -49, 21, 30, 30 };
        System.out.println("排序之前：\n" + java.util.Arrays.toString(data));
        bubbleSort(data);
        System.out.println("排序之后：\n" + java.util.Arrays.toString(data));
    }
}
```

#### 二分查找-折半查找

> 前提有序列表

```scala
def binarySearch(arr: Array[Int], left: Int, right: Int, searchVal: Int): Int = {
  if (left > right) {
    // 找不到 返回-1
    return -1;
  }
  val midIndex: Int = left + (right - left) / 2 // 防止越界
  if (arr(midIndex) == searchVal){
    // 找到返回下标
    midIndex;
  }else if(arr(midIndex) > searchVal){
    binarySearch(arr,left,midIndex,searchVal)
  }else {
    binarySearch(arr,midIndex,right,searchVal)
  }
}
```

